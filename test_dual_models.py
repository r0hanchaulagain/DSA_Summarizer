#!/usr/bin/env python3
"""Test script for the dual-model system (Gemini + Ollama fallback)."""

import os
import sys
from pathlib import Path

# Add project root to path
sys.path.append(str(Path(__file__).parent))

from src.ai_engine.local_llm import local_llm_manager
from src.utils.config import config

def test_dual_model_system():
    """Test the dual-model system."""
    print("ğŸ§ª Testing Dual-Model System")
    print("=" * 50)
    
    # Check configuration
    print(f"ğŸ”‘ Gemini API Key: {'âœ… Configured' if config.GEMINI_API_KEY else 'âŒ Not configured'}")
    print(f"ğŸ¤– Ollama Base URL: {config.OLLAMA_BASE_URL}")
    print(f"ğŸ“¦ Ollama Model: {config.OLLAMA_MODEL}")
    
    print("\nğŸ“Š Model Information:")
    model_info = local_llm_manager.get_model_info()
    for key, value in model_info.items():
        print(f"  {key}: {value}")
    
    print("\nğŸ§  Testing Response Generation:")
    
    # Test prompt
    test_prompt = "Explain what a binary search tree is in simple terms."
    test_context = "This is a video about data structures and algorithms."
    
    try:
        print(f"ğŸ“ Prompt: {test_prompt}")
        print(f"ğŸ“š Context: {test_context}")
        print("\nâ³ Generating response...")
        
        response = local_llm_manager.generate_response(test_prompt, test_context)
        
        print(f"\nâœ… Response generated successfully!")
        print(f"ğŸ¤– Model used: {type(local_llm_manager.current_model).__name__}")
        print(f"ğŸ“ Response length: {len(response)} characters")
        print(f"\nğŸ“„ Response preview:")
        print("-" * 40)
        print(response[:300] + "..." if len(response) > 300 else response)
        print("-" * 40)
        
    except Exception as e:
        print(f"\nâŒ Error generating response: {e}")
        print(f"ğŸ” Error type: {type(e).__name__}")
    
    print("\nğŸ” Testing Model Availability:")
    for i, model in enumerate(local_llm_manager.llm_models):
        model_name = type(model).__name__
        is_available = model.is_available()
        status = "âœ… Available" if is_available else "âŒ Not Available"
        print(f"  {i+1}. {model_name}: {status}")
        
        if hasattr(model, 'get_health_status'):
            try:
                health = model.get_health_status()
                if 'status' in health:
                    print(f"     Status: {health['status']}")
            except Exception:
                pass

def test_fallback_behavior():
    """Test fallback behavior when primary model fails."""
    print("\nğŸ”„ Testing Fallback Behavior:")
    print("=" * 50)
    
    # Test with a complex prompt that might trigger fallback
    complex_prompt = """
    Please provide a detailed analysis of the time complexity of the following algorithms:
    1. Binary Search
    2. Merge Sort
    3. Quick Sort
    4. Dijkstra's Algorithm
    
    Include Big O notation, space complexity, and real-world examples.
    """
    
    try:
        print("ğŸ“ Testing with complex prompt...")
        response = local_llm_manager.generate_response(complex_prompt, "")
        
        print(f"âœ… Complex prompt handled successfully!")
        print(f"ğŸ¤– Final model used: {type(local_llm_manager.current_model).__name__}")
        print(f"ğŸ“ Response length: {len(response)} characters")
        
    except Exception as e:
        print(f"âŒ Complex prompt failed: {e}")

if __name__ == "__main__":
    print("ğŸš€ DSA Video Summarizer - Dual-Model Test")
    print("=" * 60)
    
    try:
        test_dual_model_system()
        test_fallback_behavior()
        
        print("\nğŸ‰ Test completed!")
        
    except Exception as e:
        print(f"\nğŸ’¥ Test failed with error: {e}")
        import traceback
        traceback.print_exc()
