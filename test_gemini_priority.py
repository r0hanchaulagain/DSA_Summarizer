#!/usr/bin/env python3
"""Test script to verify Gemini priority in the dual-model system."""

import os
import sys
from pathlib import Path

# Add project root to path
sys.path.append(str(Path(__file__).parent))

from src.ai_engine.local_llm import local_llm_manager

def test_gemini_priority():
    """Test that Gemini is properly prioritized when available."""
    print("ğŸ§ª Testing Gemini Priority in Dual-Model System")
    print("=" * 60)
    
    try:
        # Get model information
        model_info = local_llm_manager.get_model_info()
        
        print("ğŸ“Š Current Model Configuration:")
        for key, value in model_info.items():
            if key != "model_details":  # Skip the detailed list for cleaner output
                print(f"  {key}: {value}")
        
        print(f"\nğŸ” Detailed Model Status:")
        print(local_llm_manager.debug_model_status())
        
        # Test response generation
        print(f"\nğŸ§  Testing Response Generation Priority:")
        test_prompt = "Explain what a binary search tree is in simple terms."
        
        print(f"ğŸ“ Prompt: {test_prompt}")
        print("â³ Generating response...")
        
        response = local_llm_manager.generate_response(test_prompt, "")
        
        print(f"âœ… Response generated successfully!")
        print(f"ğŸ¤– Model used: {type(local_llm_manager.current_model).__name__}")
        
        # Check if the right model was used
        if isinstance(local_llm_manager.current_model, local_llm_manager.llm_models[0].__class__):
            print("ğŸ¯ âœ… First priority model was used correctly!")
        else:
            print("âš ï¸ âš ï¸ First priority model was NOT used - check fallback logic")
        
        print(f"ğŸ“ Response length: {len(response)} characters")
        print(f"ğŸ“„ Response preview: {response[:200]}...")
        
        # Test multiple queries to see if priority is maintained
        print(f"\nğŸ”„ Testing Multiple Queries for Priority Consistency:")
        queries = [
            "What is time complexity?",
            "Explain recursion briefly",
            "What are data structures?"
        ]
        
        for i, query in enumerate(queries, 1):
            print(f"\n  Query {i}: {query}")
            try:
                response = local_llm_manager.generate_response(query, "")
                model_used = type(local_llm_manager.current_model).__name__
                print(f"    âœ… Response generated with {model_used}")
            except Exception as e:
                print(f"    âŒ Failed: {e}")
        
        print(f"\nğŸ¯ Final Model Status:")
        print(f"  Current Model: {type(local_llm_manager.current_model).__name__}")
        print(f"  Model Priority: {[type(m).__name__ for m in local_llm_manager.llm_models]}")
        
    except Exception as e:
        print(f"âŒ Test failed: {e}")
        import traceback
        traceback.print_exc()

def check_environment_variables():
    """Check environment variables that affect model selection."""
    print("\nğŸŒ Environment Variables Check:")
    print("=" * 40)
    
    gemini_key = os.getenv("GEMINI_API_KEY")
    if gemini_key:
        print(f"âœ… GEMINI_API_KEY: {gemini_key[:10]}...{gemini_key[-4:]}")
        print(f"   Length: {len(gemini_key)} characters")
    else:
        print("âŒ GEMINI_API_KEY: Not set")
        print("   ğŸ’¡ Set this to enable Gemini as primary model")
    
    ollama_url = os.getenv("OLLAMA_BASE_URL", "http://localhost:11434")
    print(f"ğŸŒ OLLAMA_BASE_URL: {ollama_url}")
    
    ollama_model = os.getenv("OLLAMA_MODEL", "codellama:7b")
    print(f"ğŸ¤– OLLAMA_MODEL: {ollama_model}")

if __name__ == "__main__":
    print("ğŸš€ DSA Video Summarizer - Gemini Priority Test")
    print("=" * 70)
    
    try:
        check_environment_variables()
        test_gemini_priority()
        
        print("\nğŸ‰ Priority test completed!")
        print("\nğŸ’¡ Expected Behavior:")
        print("   â€¢ With Gemini API key: Gemini should be used first")
        print("   â€¢ Without Gemini API key: Ollama should be used first")
        print("   â€¢ Fallback should work automatically if primary model fails")
        
    except Exception as e:
        print(f"\nğŸ’¥ Test failed with error: {e}")
        import traceback
        traceback.print_exc()
